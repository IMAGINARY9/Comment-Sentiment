# Configuration for DistilBERT sentiment analysis (fast, lightweight transformer)
model:
  name: "distilbert-base-uncased"
  num_labels: 3
  max_length: 256
  dropout: 0.1
  type: "transformer"

training:
  batch_size: 32
  learning_rate: 3e-5
  num_epochs: 4
  warmup_steps: 50
  weight_decay: 0.01
  save_steps: 500
  eval_steps: 250
  logging_steps: 50
  patience: 2

data:
  train_size: 0.8
  val_size: 0.1
  test_size: 0.1
  text_column: "text"
  label_column: "sentiment"
  platform: "twitter"

preprocessing:
  clean_text: true
  handle_mentions: true
  handle_hashtags: true
  handle_urls: true
  handle_emojis: "convert"
  expand_contractions: true
  correct_spelling: false
  remove_stopwords: false
  normalize_case: true

evaluation:
  metrics: ["accuracy", "f1", "precision", "recall"]
  average: "weighted"
