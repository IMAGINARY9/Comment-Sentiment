"""
Comprehensive test suite for the comment sentiment analysis project.
Tests social media-specific models, preprocessing, training, and evaluation.
"""

import os
import sys
import unittest
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock

import torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

try:
    from models import TwitterRoBERTaModel, BiLSTMGloVeModel, EnsembleModel, CommentSentimentPredictor
    from training import CommentTrainer, MultiPlatformTrainer
    from evaluation import CommentEvaluator, SocialMediaAnalyzer
except ImportError as e:
    print(f"Warning: Could not import some modules: {e}")
    print("Some tests may be skipped")


class TestCommentModels(unittest.TestCase):
    """Test comment sentiment models."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.device = torch.device('cpu')
        self.num_classes = 3
        self.max_length = 128
        
    def test_twitter_roberta_model_creation(self):
        """Test Twitter RoBERTa model creation."""
        try:
            model = TwitterRoBERTaModel(
                model_name="cardiffnlp/twitter-roberta-base-sentiment-latest",
                num_classes=self.num_classes,
                dropout=0.1,
                freeze_base=False
            )
            
            self.assertIsInstance(model, TwitterRoBERTaModel)
            self.assertEqual(model.num_classes, self.num_classes)
            
        except NameError:
            self.skipTest("TwitterRoBERTaModel not available")
    
    def test_bilstm_glove_model_creation(self):
        """Test BiLSTM GloVe model creation."""
        try:
            vocab_size = 10000
            embedding_dim = 100
            hidden_dim = 128
            
            model = BiLSTMGloVeModel(
                vocab_size=vocab_size,
                embedding_dim=embedding_dim,
                hidden_dim=hidden_dim,
                num_classes=self.num_classes,
                num_layers=2,
                dropout=0.3,
                use_attention=True
            )
            
            self.assertIsInstance(model, BiLSTMGloVeModel)
            self.assertEqual(model.num_classes, self.num_classes)
            
            # Test forward pass
            batch_size = 4
            seq_len = 50
            input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
            
            with torch.no_grad():
                output = model(input_ids)
                self.assertEqual(output.shape, (batch_size, self.num_classes))
                
        except NameError:
            self.skipTest("BiLSTMGloVeModel not available")
    
    def test_ensemble_model(self):
        """Test ensemble model."""
        try:
            # Create mock models
            roberta_model = Mock()
            bilstm_model = Mock()
            
            roberta_model.return_value = torch.randn(2, self.num_classes)
            bilstm_model.return_value = torch.randn(2, self.num_classes)
            
            ensemble = EnsembleModel(
                models=[roberta_model, bilstm_model],
                weights=[0.7, 0.3]
            )
            
            # Test forward pass
            input_data = {"input_ids": torch.randint(0, 100, (2, 50))}
            with torch.no_grad():
                output = ensemble(input_data)
                self.assertEqual(output.shape[1], self.num_classes)
                
        except NameError:
            self.skipTest("EnsembleModel not available")
    
    def test_comment_sentiment_predictor(self):
        """Test comment sentiment predictor."""
        try:
            mock_model = Mock()
            mock_tokenizer = Mock()
            mock_preprocessor = Mock()
            
            predictor = CommentSentimentPredictor(
                model=mock_model,
                tokenizer=mock_tokenizer,
                preprocessor=mock_preprocessor,
                label_names=['negative', 'neutral', 'positive'],
                device=self.device
            )
            
            self.assertIsInstance(predictor, CommentSentimentPredictor)
            
        except NameError:
            self.skipTest("CommentSentimentPredictor not available")


class TestCommentTraining(unittest.TestCase):
    """Test comment training functionality."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.device = torch.device('cpu')
        
        # Create mock model
        self.mock_model = Mock()
        self.mock_model.parameters.return_value = []
        self.mock_model.train = Mock()
        self.mock_model.eval = Mock()
        
    def test_comment_trainer_creation(self):
        """Test trainer creation."""
        try:
            trainer = CommentTrainer(
                model=self.mock_model,
                device=self.device,
                learning_rate=2e-5,
                weight_decay=0.01,
                warmup_steps=100
            )
            
            self.assertIsInstance(trainer, CommentTrainer)
            
        except NameError:
            self.skipTest("CommentTrainer not available")
    
    def test_multi_platform_trainer_creation(self):
        """Test multi-platform trainer creation."""
        try:
            trainer = MultiPlatformTrainer(
                model=self.mock_model,
                device=self.device,
                learning_rate=2e-5,
                platforms=['twitter', 'reddit', 'youtube'],
                platform_weights={'twitter': 0.5, 'reddit': 0.3, 'youtube': 0.2}
            )
            
            self.assertIsInstance(trainer, MultiPlatformTrainer)
            
        except NameError:
            self.skipTest("MultiPlatformTrainer not available")
    
    def test_domain_adaptation(self):
        """Test domain adaptation functionality."""
        try:
            trainer = MultiPlatformTrainer(
                model=self.mock_model,
                device=self.device,
                learning_rate=2e-5,
                platforms=['twitter', 'reddit'],
                use_domain_adaptation=True
            )
            
            # Test that domain adaptation is configured
            self.assertTrue(hasattr(trainer, 'use_domain_adaptation'))
            
        except (NameError, AttributeError):
            self.skipTest("Domain adaptation not available")


class TestCommentEvaluation(unittest.TestCase):
    """Test comment evaluation functionality."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.sample_predictions = np.array([0, 1, 2, 1, 0])
        self.sample_labels = np.array([0, 1, 1, 1, 0])
        self.sample_confidences = np.array([0.9, 0.7, 0.6, 0.8, 0.95])
        
    def test_comment_evaluator_creation(self):
        """Test evaluator creation."""
        try:
            evaluator = CommentEvaluator(
                label_names=['negative', 'neutral', 'positive'],
                output_dir="./test_outputs"
            )
            
            self.assertIsInstance(evaluator, CommentEvaluator)
            
        except NameError:
            self.skipTest("CommentEvaluator not available")
    
    def test_social_media_analyzer_creation(self):
        """Test social media analyzer creation."""
        try:
            analyzer = SocialMediaAnalyzer(
                platforms=['twitter', 'reddit', 'youtube'],
                output_dir="./test_outputs"
            )
            
            self.assertIsInstance(analyzer, SocialMediaAnalyzer)
            
        except NameError:
            self.skipTest("SocialMediaAnalyzer not available")
    
    def test_confidence_analysis(self):
        """Test confidence analysis."""
        try:
            evaluator = CommentEvaluator(
                label_names=['negative', 'neutral', 'positive']
            )
            
            analysis = evaluator.analyze_confidence(
                predictions=self.sample_predictions,
                confidences=self.sample_confidences,
                labels=self.sample_labels
            )
            
            self.assertIsInstance(analysis, dict)
            
        except (NameError, AttributeError):
            self.skipTest("Confidence analysis not available")
    
    def test_error_analysis(self):
        """Test error analysis."""
        try:
            evaluator = CommentEvaluator(
                label_names=['negative', 'neutral', 'positive']
            )
            
            # Mock text data
            texts = ["sample text"] * len(self.sample_predictions)
            
            errors = evaluator.analyze_errors(
                predictions=self.sample_predictions,
                labels=self.sample_labels,
                texts=texts,
                confidences=self.sample_confidences
            )
            
            self.assertIsInstance(errors, dict)
            
        except (NameError, AttributeError):
            self.skipTest("Error analysis not available")
    
    def test_platform_specific_metrics(self):
        """Test platform-specific metrics."""
        try:
            analyzer = SocialMediaAnalyzer(
                platforms=['twitter', 'reddit', 'youtube']
            )
            
            # Mock platform data
            platform_predictions = {
                'twitter': self.sample_predictions[:2],
                'reddit': self.sample_predictions[2:4],
                'youtube': self.sample_predictions[4:]
            }
            
            platform_labels = {
                'twitter': self.sample_labels[:2],
                'reddit': self.sample_labels[2:4],
                'youtube': self.sample_labels[4:]
            }
            
            metrics = analyzer.calculate_platform_metrics(
                platform_predictions, platform_labels
            )
            
            self.assertIsInstance(metrics, dict)
            
        except (NameError, AttributeError):
            self.skipTest("Platform-specific metrics not available")


class TestIntegration(unittest.TestCase):
    """Integration tests for the comment sentiment system."""
    
    def test_multi_platform_training_pipeline(self):
        """Test multi-platform training pipeline."""
        try:
            # Mock multi-platform dataset
            platforms = ['twitter', 'reddit', 'youtube']
            
            # Create mock data for each platform
            platform_datasets = {}
            for platform in platforms:
                # Create small mock dataset
                input_ids = torch.randint(0, 1000, (10, 50))
                attention_masks = torch.ones(10, 50)
                labels = torch.randint(0, 3, (10,))
                
                dataset = TensorDataset(input_ids, attention_masks, labels)
                platform_datasets[platform] = DataLoader(dataset, batch_size=4)
            
            self.assertEqual(len(platform_datasets), 3)
            
            # Test that each platform has a dataloader
            for platform, dataloader in platform_datasets.items():
                self.assertIsInstance(dataloader, DataLoader)
                
        except Exception as e:
            self.skipTest(f"Multi-platform test skipped due to: {e}")


class TestUtilities(unittest.TestCase):
    """Test utility functions."""
    
    def test_emoji_handling(self):
        """Test emoji handling utilities."""
        try:
            # Test emoji detection
            text_with_emoji = "I love this! üòçüòäüëç"
            
            # If emoji handling is implemented
            # This would test the actual implementation
            self.assertIn("üòç", text_with_emoji)
            
        except Exception as e:
            self.skipTest(f"Emoji handling test skipped: {e}")
    
    def test_hashtag_extraction(self):
        """Test hashtag extraction."""
        try:
            text_with_hashtags = "This is #awesome and #great #amazing"
            
            # If hashtag extraction is implemented
            # This would test the actual implementation
            self.assertIn("#awesome", text_with_hashtags)
            
        except Exception as e:
            self.skipTest(f"Hashtag extraction test skipped: {e}")
    
    def test_mention_handling(self):
        """Test mention handling."""
        try:
            text_with_mentions = "Thanks @user1 and @user2 for helping!"
            
            # If mention handling is implemented
            # This would test the actual implementation
            self.assertIn("@user1", text_with_mentions)
            
        except Exception as e:
            self.skipTest(f"Mention handling test skipped: {e}")


if __name__ == '__main__':
    # Run tests
    unittest.main(verbosity=2)
